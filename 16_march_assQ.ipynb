{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3444c3f4",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2279b8",
   "metadata": {},
   "source": [
    "Overfitting: \n",
    "\n",
    "         When a model performs very well for training data but has poor performance with test data (new data), it is known as overfitting. In this case, the machine learning model learns the details and noise in the training data such that it negatively affects the performance of the model on test data. \n",
    "         Overfitting can happen due to low bias and high variance.\n",
    "\n",
    "Reasons for Overfitting are as follows:\n",
    "\n",
    "1) Data used for training is not cleaned and contains noise (garbage values) in it\n",
    "2) The model has a high variance\n",
    "3) The size of the training dataset used is not enough\n",
    "4) The model is too complex \n",
    "\n",
    "Ways to Tackle Overfitting\n",
    "\n",
    "1) Using K-fold cross-validation\n",
    "2) Using Regularization techniques such as Lasso and Ridge\n",
    "3) Training model with sufficient data\n",
    "4) Adopting ensembling techniques.\n",
    "         \n",
    "Underfitting:\n",
    "            When a model has not learned the patterns in the training data well and is unable to generalize well on the new data, it is known as underfitting. An underfit model has poor performance on the training data and will result in unreliable predictions. \n",
    "            \n",
    "                 Underfitting occurs due to high bias and high variance.\n",
    "            \n",
    "Reasons for Underfitting:\n",
    "\n",
    "1) Data used for training is not cleaned and contains noise (garbage values) in it\n",
    "2) The model has a high bias\n",
    "3) The size of the training dataset used is not enough\n",
    "4) The model is too simple\n",
    "\n",
    "\n",
    "Ways to Tackle Underfitting:\n",
    "\n",
    "1) Increase the number of features in the dataset\n",
    "2) Increase model complexity\n",
    "3) Reduce noise in the data\n",
    "4) Increase the duration of training the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b4237",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2946ec",
   "metadata": {},
   "source": [
    "Example of Overfitting:\n",
    "\n",
    "Let’s say we want to predict if a student will land a job interview based on her resume.\n",
    "Now, assume we train a model from a dataset of 10,000 resumes and their outcomes.\n",
    "Next, we try the model out on the original dataset, and it predicts outcomes with 99% accuracy… wow!\n",
    "But when we run the model on a new (“unseen”) dataset of resumes, we only get 50% accuracy.\n",
    "Our model doesn’t generalize well from our training data to unseen data.\n",
    "When if our model does much better on the training set than on the test set, then we’re likely overfitting.\n",
    "\n",
    "For example, it would be a big red flag if our model saw 99% accuracy on the training set but only 55% accuracy on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e255d5f",
   "metadata": {},
   "source": [
    "How to Prevent Overfitting in Machine Learning:\n",
    "\n",
    "Cross-validation:\n",
    "   \n",
    "Cross-validation is a powerful preventative measure against overfitting.\n",
    "Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n",
    "In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”).\n",
    "\n",
    "Cross-validation allows you to tune hyperparameters with only your original training set. \n",
    "This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
    "\n",
    "Train with sufficient data:\n",
    "\n",
    "It won’t work every time, but training with more data can help algorithms detect the signal better.\n",
    "Of course, that’s not always the case. If we just add more noisy data, this technique won’t help. That’s why you should always ensure your data is clean and relevant.\n",
    "\n",
    "Remove features:\n",
    "\n",
    "Some algorithms have built-in feature selection.\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "There are several feature selection heuristics you can use.\n",
    "\n",
    "Early stopping:\n",
    "\n",
    "When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "Early stopping refers stopping the training process before the learner passes that point.\n",
    "\n",
    "Regularization:\n",
    "\n",
    "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "The egularization method is a hyperparameter as well, which means it can be tuned through cross-validation.\n",
    "\n",
    "Ensembling:\n",
    "\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "    a) It trains a large number of “strong” learners in parallel.\n",
    "    b) A strong learner is a model that’s relatively unconstrained.\n",
    "    c) Bagging then combines all the strong learners together in order to “smooth out” their predictions.\n",
    "    \n",
    "Boosting attempts to improve the predictive flexibility of simple models.\n",
    "\n",
    "    a) It trains a large number of “weak” learners in sequence.\n",
    "    b) A weak learner is a constrained model (i.e. you could limit the max depth of each decision    tree).\n",
    "    c) Each one in the sequence focuses on learning from the mistakes of the one before it.\n",
    "    d) Boosting then combines all the weak learners into a single strong learner.\n",
    "    \n",
    "While bagging and boosting are both ensemble methods, they approach the problem from opposite directions.\n",
    "\n",
    "Bagging uses complex base models and tries to “smooth out” their predictions, while boosting uses simple base models and tries to “boost” their aggregate complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b47b98",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec42c97",
   "metadata": {},
   "source": [
    "Underfitting:\n",
    "            Underfitting is a common problem in machine learning where a model is not able to capture the underlying patterns in the training data and therefore performs poorly on both the training data and new, unseen data. \n",
    "            In other words, the model is too simple to represent the complexity of the data and fails to capture important relationships between the input and output variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c2a26",
   "metadata": {},
   "source": [
    "List of scenario where underfitteng can occur:\n",
    "\n",
    "1) Model complexity: \n",
    "            underfitted models don’t effectively capture the relationship between the input and output data because it is too simple.\n",
    "            \n",
    "2) Insufficient training data:\n",
    "            A model may underfit the data if there is not enough training data to capture the underlying patterns. In such cases, the model may generalize poorly to new, unseen data.\n",
    " \n",
    "3) Feature selection:\n",
    "            If we select features that are not relevant or informative, the model may not be able to capture the underlying patterns in the data and may underfit the data.\n",
    "            \n",
    "4) Regularization:\n",
    "            Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function, which discourages the model from using overly complex solutions. However, if the regularization parameter is set too high, the model may become too simple and underfit the data.\n",
    "            \n",
    "5) Preprocessing:\n",
    "            Preprocessing the data before training the model is important to ensure that the data is in an appropriate format and that any outliers or noise are removed. If the data is not preprocessed properly, the model may underfit the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb103e",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d824e4",
   "metadata": {},
   "source": [
    "To make predictions, our model will analyze our data and find patterns in it. Using these patterns, we can make generalizations about certain instances in our data. \n",
    "Our model job is to learn from the data i.e training data applies them to the test set to predict them.\n",
    "\n",
    "Bias:\n",
    "        Bias is the difference between our actual and predicted values. Bias is the simple assumptions that our model makes about our data to be able to predict new data.\n",
    "        During training, it allows our model to ‘see’ the data a certain number of times to find patterns in it. If it does not work on the data for long enough, it will not find patterns and bias occurs. \n",
    "        \n",
    "Low Bias : Low bias means diffrence between predicted and actual value is low i.e our model is capturing the training data very well and learning from them is also very good.i.e we are predicting very good way.\n",
    "\n",
    "        line of best fit is a straight line that  pass through most of the data points\n",
    "\n",
    "High Bias :  Here difference between actual value and predicted value having far difference.\n",
    "    This ia caused because assumptions made by our model are too basic, the model can’t capture the important features of our data. This means that our model hasn’t captured patterns in the training data and hence cannot perform well on the testing data too. If this is the case, our model cannot perform on new data and cannot be sent into production. \n",
    "    \n",
    "       line of best fit is a straight line that does not pass through any of the data points.\n",
    "       High bias occur underfitting.\n",
    "    \n",
    "Variance:\n",
    "\n",
    " Variance is the very opposite of Bias. On the other hand, if our model is allowed to view the data too many times, it will learn very well for only that data. It will capture most patterns in the data,  but it will also learn from the unnecessary data present, or from the noise.\n",
    "\n",
    "    We can define variance as the model’s sensitivity to fluctuations in the data. Our model may learn from noise. This will cause our model to consider trivial features as important\n",
    "    \n",
    "    Example : our model has learned extremely well for our training data, which has taught it to identify cats. But when given new data, such as the picture of a fox, our model predicts it as a cat, as that is what it has learned. This happens when the Variance is high, our model will capture all the features of the data given to it, including the noise, will tune itself to the data, and predict it very well but when given new data, it cannot predict on it as it is too specific to training data. \n",
    "    \n",
    "    i.e high varience tries to fit each and every data point to the best fit line so it causes overfitting.\n",
    "    \n",
    "Low Varience :\n",
    "            It is trained with the sufficient data so the model will give good accuracy on test data.\n",
    "            \n",
    "High Varience :\n",
    "            It is trained too well including noise so it tries to fit each and every data point of the sample and caused high varience.\n",
    "            \n",
    "bias-variance tradeoff:\n",
    " \n",
    "For any model, we have to find the perfect balance between Bias and Variance. \n",
    "This just ensures that we capture the essential patterns in our model while ignoring the noise present it in. This is called Bias-Variance Tradeoff. \n",
    "It helps optimize the error in our model and keeps it as low as possible.\n",
    "\n",
    "An optimized model will be sensitive to the patterns in our data, but at the same time will be able to generalize to new data. In this, both the bias and variance should be low so as to prevent overfitting and underfitting.\n",
    "        We need low bias and low varience.\\\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1954879c",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc5111f",
   "metadata": {},
   "source": [
    "The easiest way to detect overfitting is to perform cross-validation.\n",
    "The most commonly used method is known as k-fold cross validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dde6c",
   "metadata": {},
   "source": [
    "How to check if the model is overfitting or underfitting:\n",
    "\n",
    "There are several ways to detect over- or under-fitting in a machine learning model:\n",
    "\n",
    "Plot the learning curves: \n",
    "\n",
    "        Learning curves show the model’s performance on training and validation data over time as the model is being trained. If the model is overfitting, you will see that the training error continues to decrease over time, while the validation error starts to increase after a certain point. This indicates that the model is beginning to memorise the training data and needs to be generalised well to new, unseen data.\n",
    "\n",
    "Evaluate the model on a holdout set: \n",
    "\n",
    "        A holdout set is a subset of the data that is not used during training but is used to evaluate the model after training. If the model performs well on the training data but poorly on the holdout set, it may be overfitting the training data.\n",
    "        \n",
    "Use cross-validation: \n",
    "\n",
    "             Cross-validation is a technique where the data is divided into k-folds, and the model is trained and evaluated on each fold. If the model performs well on the training data but poorly on the validation data, it may need to be more balanced.\n",
    "             \n",
    "Regularise the model: \n",
    "\n",
    "    Regularization is a method that adds a penalty term to the loss function to stop the model from becoming too similar to the training data. By changing the regularisation parameter, you can control how hard the model is to understand and prevent it from becoming too simple.\n",
    "    \n",
    "Use simpler models:\n",
    "\n",
    "        If your complex model is overfitting the data, you can use simpler models less prone to overfitting, such as linear models or decision trees with low depth.\n",
    "        \n",
    "    In general, it’s crucial to monitor the model’s performance during training and evaluation and to be aware of the trade-off between model complexity and generalisation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e3d3f",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283e7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                     Bias                               Varience\n",
    "    \n",
    "1) Model Complexity         High model complexity tend              High model complexity tend         \n",
    "                                to low bias                         to high varience\n",
    "    \n",
    "2) Parametric and linear     High bias                               Low Varience\n",
    "    Algorithm \n",
    "     \n",
    "3)Non Parametric and            Low Bias                                   High Varience   \n",
    "    Non linear algo\n",
    "    \n",
    "4)Causes                    High bias:Underfitting                  High Varience:Overfitting\n",
    "\n",
    "5)Example                    Low Bias :KNN Decision Tree           Low Varience:Linear regression\n",
    "                             High bias :Linear Regression           High Varience : SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "some examples of high bias and high variance models:\n",
    "\n",
    "Algorithm                        Bias                                  Variance\n",
    "\n",
    "Linear Regression                High Bias                       Less Variance\n",
    "Decision Tree                    Low Bias                        High Variance\n",
    "Bagging                          Low Bias                        High Variance (Less than Decision Tree)\n",
    "Random Forest                    Low Bias                        High Variance (Less than Decision Tree and Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb020b86",
   "metadata": {},
   "source": [
    "**high bias and high variance models, and how do they differ in terms of their performance**\n",
    "\n",
    "A high bias model is one that has oversimplified the problem and has high bias towards a particular hypothesis. Such models typically underfit the data and may have low accuracy on both the training and testing datasets. They may miss important patterns or relationships in the data and make very basic assumptions about the data. For example, a linear regression model may have high bias if it tries to fit a linear line to a dataset that has a complex nonlinear relationship. This model will have low training accuracy and will also have low accuracy on new data.\n",
    "\n",
    "A high variance model, on the other hand, is one that is overly complex and has high variance towards the training data. Such models typically overfit the data and have high accuracy on the training dataset, but may have poor accuracy on new data. They may fit noise in the data and are very sensitive to small fluctuations in the training dataset. For example, a decision tree with a large number of branches can be an example of a high variance model. This model can fit the training data very well, but can have poor accuracy on new data due to overfitting.\n",
    "\n",
    "In general, a model that has high bias has low variance and is less sensitive to changes in the training data. Such models may be underfitting the data and need more complexity to fit the data well. A model that has high variance has low bias and is more sensitive to changes in the training data. Such models may be overfitting the data and need regularization techniques to reduce variance.\n",
    "\n",
    "In terms of performance, a high bias model will have low training and testing accuracy, while a high variance model will have high training accuracy and low testing accuracy. The ideal model is one that has low bias and low variance, which means that it fits the data well and generalizes well to new data. Achieving this balance is known as the bias-variance tradeoff, and finding the optimal balance can be a challenging task in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f17b1",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd092d4",
   "metadata": {},
   "source": [
    "With the hehp of regularization we can solve few common issues:\n",
    "\n",
    "    1)minimizing model complexity\n",
    "    2)Penalizing the loss function\n",
    "    3)Reducing model overfitting\n",
    "         \n",
    "Regularization is a technique used to reduce the errors by fitting the function appropriately on the given training set and avoid overfitting.\n",
    "\n",
    "Regularization is a technique that penalizes the coefficient.\n",
    "In an overfit model, the coefficients are generally inflated.\n",
    "Thus, Regularization adds penalties to the parameters and avoids them weigh heavily.\n",
    "\n",
    "The coefficients are added to the cost function of the linear equation. Thus, if the coefficient inflates, the cost function will increase. And Linear regression model will try to optimize the coefficient in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5720c7f",
   "metadata": {},
   "source": [
    "The commonly used regularization techniques are : \n",
    "\n",
    "1) L2 regularization or Ridge Regression:\n",
    "        \n",
    "         In this model we have high varience and low bias as traning data it fir very well but varience is high while testing the dataset.so This regression model is overfit in nature.\n",
    "         \n",
    "         Solution: here we are incresing the bias and lowering the varience.This will done by changing the slope(best fit line)\n",
    "          here model performance is little poor in case of training set but it will consistance inprove on the testing data set.\n",
    "          \n",
    "          Here slope has been reduced with ridge regression penalty and therefore model become less sensetive to changes in independent variable.\n",
    "          L2 Regularization technique is also known as Ridge. In this, the penalty term added to the cost function is the summation of the squared value of coefficients. Unlike the LASSO term, the Ridge term uses squared values of the coefficient and can reduce the coefficient value near to 0 but not exactly 0. Ridge distributes the coefficient value across all the features.\n",
    "          \n",
    "             Least Sqaured Regression =Min (sum of squared residuals)\n",
    "             \n",
    "             \n",
    "             Ridge Regression = Min (sum of squared residuals + alpha*slope**2       \n",
    "                 \n",
    "                          alpha*slope**2= penalty term\n",
    "                          \n",
    "          Alpha effect :1)Alpha increases the slope of rgression line is reduced and become more horizontal\n",
    "                        2) Alpha increases become less sensetive to variation of independent variable\n",
    "2) L1 regularization or LASSO  regularization:\n",
    "        \n",
    "        Losso is same as ridge regression but here bias term is absolute value of slope is added as penalty term\n",
    "        \n",
    "                    LOSSO Regression = Min (sum of squared residuals + alpha*|slope|) \n",
    "                 \n",
    "                          alpha*slope**2= penalty term\n",
    "                          \n",
    "          Effect of LOSSO regression is same as Ridge regression.\n",
    "          \n",
    "          Alpha effect :1)Alpha increases the slope of rgression line is reduced and become more horizontal\n",
    "                        2) Alpha increases become less sensetive to variation of independent variable\n",
    "                        \n",
    "                        \n",
    "        L1 Regularization technique is also known as LASSO or Least Absolute Shrinkage and Selection Operator. In this, the penalty term added to the cost function is the summation of absolute values of the coefficients. Since the absolute value of the coefficients is used, it can reduce the coefficient to 0 and such features may completely get discarded in LASSO. Thus, we can say, LASSO helps in Regularization as well as Feature Selection.\n",
    "    \n",
    "\n",
    "        \n",
    "3) ElasticNet Regression:\n",
    "\n",
    "        ElasticNet Regression is a linear model built by applying both L1 and L2 penalty terms. \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed415dd",
   "metadata": {},
   "source": [
    "1) L1 regularization or LASSO  regularization:\n",
    "\n",
    "       Lasso Regression adds “absolute value of magnitude” of coefficient as penalty term to the loss function(L). \n",
    "       \n",
    "        ||W||1 = |W1|+|W2|+|W3|+...+|Wn|\n",
    "        \n",
    "        \n",
    "2) L2 regularization or Ridge Regression:\n",
    "\n",
    "        Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function(L). \n",
    "        \n",
    "    ||W||2 = (|W1|**2+|W2|**2+|W3|**2+...+|Wn|**2)**0.5\n",
    "    \n",
    "\n",
    "**NOTE that during Regularization the output function(y_hat) does not change. The change is only in the loss function. \n",
    "\n",
    "The output function is :\n",
    "\n",
    "    y^ =w1x1+w2x2+-------------+WnXn+b\n",
    "    \n",
    "**The loss function befor regularization \n",
    "\n",
    "        Loss = Error(y,y^)\n",
    "        \n",
    "**The loss function after regularization(L1)\n",
    "\n",
    "        Loss = Error(y,y^)+lamba(summation of 1 to N)|Wi|\n",
    "        \n",
    "**The loss function after regularization(L2)\n",
    "\n",
    "        Loss = Error(y,y^)+lamba(summation of 1 to N)|Wi|**2\n",
    "        \n",
    "**We define Loss function in Logistic Regression as : \n",
    "\n",
    "         L(y_hat,y) = y log y_hat + (1 - y)log(1 - y_hat)\n",
    "         \n",
    "**Loss function with no regularization : \n",
    "\n",
    "         L = y log (wx + b) + (1 - y)log(1 - (wx + b)) \n",
    "         \n",
    " Lets say the data overfits the above function.    \n",
    "\n",
    "\n",
    "**Loss function with L1 regularization : \n",
    "            \n",
    "            L = y log (wx + b) + (1 - y)log(1 - (wx + b)) + lambda*||w||1    \n",
    "            \n",
    "**L = y log (wx + b) + (1 - y)log(1 - (wx + b)) + lambda*||w||1    \n",
    "\n",
    "\n",
    "             L = y log (wx + b) + (1 - y)log(1 - (wx + b)) + lambda*||w||22    \n",
    "\n",
    "lambda is a Hyperparameter Known as regularization constant and it is greater than zero  \n",
    "                            lambda > 0\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

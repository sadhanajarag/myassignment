{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33fc48d",
   "metadata": {},
   "source": [
    "## Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca71e8",
   "metadata": {},
   "source": [
    "### Def:\n",
    "    Anomaly detection is a technique used in data analysis and machine learning to identify patterns or instances that deviate significantly from the expected behavior within a dataset. \n",
    "### Aim:\n",
    "    The purpose of anomaly detection is to identify observations that are considered unusual, rare, or abnormal compared to the majority of the data. These anomalies can be indicative of interesting events, errors, or potential threats within a system or process.\n",
    "\n",
    "### Example:\n",
    "    Anomalies can take various forms depending on the context. For example, in network security, anomalies might represent suspicious network traffic that could indicate a cyber attack. In manufacturing, anomalies could indicate faulty equipment or deviations from the standard production process. Anomalies could also be found in financial transactions, healthcare monitoring, fraud detection, system monitoring, and many other domains.\n",
    "\n",
    "The main goals of anomaly detection are as follows:\n",
    "\n",
    "1. **Identification of unusual events:**\n",
    "        Anomaly detection helps in identifying rare occurrences or patterns that do not conform to the expected behavior. By flagging these anomalies, it enables further investigation and appropriate action.\n",
    "\n",
    "2. **Early detection of anomalies:** \n",
    "        By detecting anomalies as soon as they occur, prompt actions can be taken to mitigate potential risks, prevent system failures, or address emerging issues before they escalate.\n",
    "\n",
    "3. **Data quality assurance:**\n",
    "            Anomaly detection can be used to ensure the integrity and quality of data by identifying outliers, errors, or missing values that can negatively impact data analysis and decision-making.\n",
    "\n",
    "4. **Security and fraud detection:**\n",
    "        Anomaly detection plays a crucial role in identifying suspicious activities, anomalies, or unauthorized access attempts that could be indicative of security breaches, fraud, or malicious behavior.\n",
    "\n",
    "5. **Performance monitoring and predictive maintenance:**\n",
    "            By monitoring systems and processes for anomalies, it becomes possible to identify performance degradation, anticipate failures, and enable proactive maintenance, thereby reducing downtime and improving efficiency.\n",
    "\n",
    "Overall, anomaly detection helps in uncovering hidden insights, enhancing system reliability, and facilitating timely decision-making across various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30497a5b",
   "metadata": {},
   "source": [
    "## Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dd9fd",
   "metadata": {},
   "source": [
    "Anomaly detection poses several challenges that need to be addressed to ensure accurate and effective results. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "### Unlabeled data:\n",
    "        Anomaly detection often deals with unlabeled data, where the majority of instances are considered normal, and anomalies are rare. Without labeled examples of anomalies, it becomes challenging to train a model to accurately distinguish between normal and abnormal instances.\n",
    "\n",
    "### 1. Imbalanced data:\n",
    "    Anomaly detection datasets typically suffer from class imbalance, where the number of normal instances significantly outweighs the number of anomalies. This makes it difficult for models to learn effectively and may lead to biased results.\n",
    "\n",
    "### 2. Dynamic environments: \n",
    "    In dynamic environments, the characteristics of normal and anomalous behavior can change over time. Anomaly detection algorithms need to adapt and be able to handle these changes to maintain accurate performance.\n",
    "\n",
    "### 3. Feature engineering:\n",
    "        Identifying informative features that effectively capture normal and anomalous patterns is crucial. However, in some cases, relevant features may be missing or hard to define, making feature engineering a complex task.\n",
    "\n",
    "### 4. Scalability: \n",
    "    Anomaly detection algorithms should be capable of handling large-scale datasets efficiently. As data volumes grow, the computational complexity of detecting anomalies can become a significant challenge.\n",
    "\n",
    "### 5. Noise and outliers:\n",
    "        Anomalies can be challenging to distinguish from noise or outliers that are not necessarily abnormal. Separating genuine anomalies from noise or outliers requires careful consideration and robust algorithms.\n",
    "\n",
    "### 6. Anomaly diversity:\n",
    "        Anomalies can manifest in various forms and have different characteristics, making it difficult to capture the full spectrum of anomalies in a single approach. Anomaly detection algorithms should be flexible enough to accommodate diverse anomaly types.\n",
    "\n",
    "### 7. Labeling anomalies:\n",
    "        In situations where historical data is available, labeling anomalies can be a labor-intensive and subjective process. Human expertise is often required to determine if an instance is genuinely anomalous or to establish ground truth for training and evaluation.\n",
    "\n",
    "### 8. Real-time detection:\n",
    "        Some applications require real-time or near real-time anomaly detection, where anomalies need to be detected and responded to immediately. Achieving low latency and high detection accuracy in real-time settings can be challenging.\n",
    "\n",
    "### How to address these challenges:\n",
    "\n",
    "-  development of advanced anomaly detection techniques, including robust algorithms, appropriate evaluation metrics, and domain-specific knowledge.\n",
    "- Researchers and practitioners continuously work on improving anomaly detection methods to overcome these challenges \n",
    "- enhance the effectiveness and reliability of anomaly detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030eaf4",
   "metadata": {},
   "source": [
    "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05021be",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection differ primarily in their approaches to training data and the availability of labeled examples. Here's a comparison of the two:\n",
    "\n",
    "### Unsupervised Anomaly Detection:\n",
    "\n",
    "- **Training Data:** Unsupervised anomaly detection algorithms operate on unlabeled data, where anomalies are not explicitly identified or labeled.\n",
    "- **Learning Approach:** Unsupervised methods aim to model the normal behavior of the data without any prior knowledge of anomalies. They rely on identifying patterns, structures, or statistical deviations in the data that are considered rare or different from the majority.\n",
    "- **Anomaly Detection:** These algorithms detect anomalies based on the assumption that anomalies are significantly different from the normal instances in the dataset. They typically identify instances that have low probability, high distance, or do not conform to the learned patterns.\n",
    "- **Applicability:** Unsupervised anomaly detection is useful when there is limited or no prior knowledge about the anomalies, or when anomalies are rare and unexpected. It is widely applicable when labeled anomaly data is scarce or expensive to obtain.\n",
    "\n",
    "### Supervised Anomaly Detection:\n",
    "\n",
    "- **Training Data:** Supervised anomaly detection algorithms require labeled data, where both normal and anomalous instances are explicitly identified and labeled.\n",
    "- **Learning Approach:** Supervised methods learn a model using the labeled training data, where the algorithm is trained to distinguish between normal and anomalous instances based on the provided labels. They aim to generalize the characteristics of anomalies based on the labeled examples.\n",
    "- **Anomaly Detection:** Once the model is trained, it can classify new instances as either normal or anomalous based on the learned patterns. It leverages the labeled training data to make predictions and determine the likelihood of an instance being an anomaly.\n",
    "- **Applicability:** Supervised anomaly detection is suitable when a sufficient amount of labeled anomaly data is available and when there is a clear understanding of the types of anomalies that need to be detected. It is effective when the anomalies have distinct features or characteristics that can be learned from labeled examples.\n",
    "\n",
    "In summary, unsupervised anomaly detection algorithms do not rely on labeled anomaly data and focus on identifying deviations or patterns that are different from the majority of the data.\n",
    "\n",
    "Supervised anomaly detection, on the other hand, requires labeled data to train a model that can differentiate between normal and anomalous instances based on the provided labels.\n",
    "\n",
    "The choice between the two approaches depends on the availability of labeled data, the nature of anomalies, and the specific requirements of the anomaly detection task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc7b45",
   "metadata": {},
   "source": [
    "## Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841707bb",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into several main categories based on their underlying principles and techniques. Here are some of the commonly used categories:\n",
    "\n",
    "### 1.Statistical Methods: \n",
    "        Statistical methods assume that the normal data follows a specific statistical distribution, such as Gaussian (normal) distribution. Deviations from this distribution are considered anomalies. Techniques like z-score, modified z-score, and Gaussian Mixture Models (GMM) are used to detect anomalies based on statistical properties of the data.\n",
    "\n",
    "### 2. Machine Learning Methods:\n",
    "        Machine learning algorithms, both supervised and unsupervised, are used for anomaly detection. Unsupervised techniques, such as clustering algorithms (e.g., k-means, DBSCAN), density-based methods (e.g., Local Outlier Factor), and one-class SVM, learn patterns from unlabeled data and identify instances that do not conform to those patterns. Supervised methods, like classification algorithms (e.g., decision trees, random forests), are trained on labeled data to distinguish between normal and anomalous instances.\n",
    "\n",
    "### 3. Neural Network-based Methods:\n",
    "        Neural networks, including deep learning models, are increasingly used for anomaly detection. Autoencoders, a type of neural network, are commonly employed for unsupervised anomaly detection. They learn to reconstruct normal instances and identify instances that have high reconstruction errors as anomalies. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are also utilized for anomaly detection in sequential data.\n",
    "\n",
    "### 4. Distance-based Methods:\n",
    "        Distance-based algorithms measure the distance or dissimilarity between instances and determine anomalies based on their distance from the majority of the data. Techniques like k-nearest neighbors (KNN) and distance-based clustering (e.g., LOF - Local Outlier Factor) fall under this category.\n",
    "\n",
    "### 5. Information Theory-based Methods:\n",
    "        Information theory-based methods quantify the information content or entropy of instances. Anomalies are identified based on the deviation from the expected information content. One example is the Minimum Description Length (MDL) principle, which seeks to minimize the code length required to describe the data.\n",
    "\n",
    "### 6. Domain-specific Methods:\n",
    "        Some domains have specific anomaly detection techniques tailored to their characteristics. For example, in network traffic analysis, techniques like anomaly-based intrusion detection systems (IDS) or behavior-based anomaly detection are used. Time series data may employ techniques like change point detection or seasonality-based anomaly detection.\n",
    "\n",
    "It's worth noting that these categories are not mutually exclusive, and hybrid approaches combining multiple techniques are also employed in anomaly detection. The choice of algorithm depends on the nature of the data, the available labeled data (if any), the complexity of anomalies, and the specific requirements of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f61e94",
   "metadata": {},
   "source": [
    "## Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101e19c",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make several key assumptions to identify anomalies based on the distance or dissimilarity between instances. Here are the main assumptions made by distance-based anomaly detection methods:\n",
    "\n",
    "#### 1. Assumption of Normality: \n",
    "        Distance-based methods assume that the majority of instances in the dataset represent normal behavior or belong to the same underlying distribution. Anomalies are expected to deviate significantly from this normal behavior.\n",
    "\n",
    "#### 2. Neighborhood Density: \n",
    "        These methods assume that normal instances are surrounded by similar instances or form dense neighborhoods. Anomalies, on the other hand, are expected to reside in sparse or less dense regions of the data space.\n",
    "\n",
    "#### 3. Distance Metric: \n",
    "        Distance-based methods rely on a distance metric to quantify the dissimilarity between instances. They assume that the chosen distance metric effectively captures the relevant characteristics of the data and can discriminate between normal and anomalous instances.\n",
    "\n",
    "#### 4. Outlier Threshold: \n",
    "        Distance-based methods assume the presence of a threshold or boundary that separates normal instances from anomalies. Instances that exceed this threshold or fall outside the boundary are considered anomalies. The determination of an appropriate threshold is crucial for accurate anomaly detection.\n",
    "\n",
    "#### 5. Euclidean Space: \n",
    "        Many distance-based methods assume that the data lies in a Euclidean space, where the concept of distance is well-defined. In cases where the data is non-Euclidean or has complex structures, additional techniques like manifold learning or kernel functions may be used to transform the data into a suitable space.\n",
    "\n",
    "It's important to note that these assumptions may not hold in all scenarios, and the effectiveness of distance-based anomaly detection methods can be influenced by the specific characteristics of the data and the nature of anomalies. Therefore, it is advisable to assess the suitability of these assumptions for a given dataset and consider alternative techniques if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6406f",
   "metadata": {},
   "source": [
    "## Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdb6cf",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores by assessing the local density of instances and comparing it to the density of their neighboring instances. The anomaly score reflects the degree to which an instance deviates from its local neighborhood. Here's a step-by-step explanation of how the LOF algorithm computes anomaly scores:\n",
    "\n",
    "### 1. Calculating Local Reachability Density (LRD):\n",
    "\n",
    "- For each instance in the dataset, the algorithm identifies its k nearest neighbors (k is a user-defined parameter).\n",
    "- It calculates the reachability distance (a measure of distance) between the instance and its neighbors.\n",
    "- The reachability distance is determined as the maximum of either the Euclidean distance between the instance and its neighbor or the reachability distance of the neighbor itself.\n",
    "- The local reachability density (LRD) of an instance is computed as the inverse of the average reachability distance of its k nearest neighbors.\n",
    "\n",
    "### 2. Calculating Local Outlier Factor (LOF):\n",
    "\n",
    "- For each instance, the LOF algorithm determines its k nearest neighbors and calculates their LRD values.\n",
    "- It computes the LRD ratio between the instance and each of its k nearest neighbors, which represents how the instance's LRD compares to its neighbors' LRD values.\n",
    "- The Local Outlier Factor (LOF) of an instance is then computed as the average of the LRD ratios of its k nearest neighbors.\n",
    "- Higher LOF values indicate that the instance is relatively more isolated or has a lower density compared to its neighbors, suggesting it is potentially an outlier or anomaly.\n",
    "\n",
    "### 3. Anomaly Scores:\n",
    "\n",
    "- The LOF algorithm assigns an anomaly score to each instance based on its LOF value. Anomalies will typically have higher LOF scores, indicating their deviation from the local density patterns observed in the data.\n",
    "- The anomaly scores can be normalized or scaled to a specific range for easier interpretation or comparison.\n",
    "- By comparing the LOF scores of instances, one can identify the anomalies that exhibit significantly higher LOF values compared to the majority of instances. Instances with LOF scores above a certain threshold are considered anomalies.\n",
    "\n",
    "The LOF algorithm is effective in identifying local anomalies that might be missed by global density-based methods. It takes into account the density variations in different regions of the dataset and provides a more nuanced measure of anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b712c",
   "metadata": {},
   "source": [
    "Let's consider a simple example to demonstrate how the LOF algorithm computes anomaly scores. Suppose we have a dataset of 10 instances represented as points in a 2-dimensional space. We will use a value of k = 3 for the nearest neighbors.\n",
    "\n",
    "The dataset:\n",
    "\n",
    "Instance\tFeature 1\tFeature 2\n",
    "A\t2\t3\n",
    "B\t4\t5\n",
    "C\t6\t7\n",
    "D\t8\t9\n",
    "E\t10\t11\n",
    "F\t12\t13\n",
    "G\t14\t15\n",
    "H\t16\t17\n",
    "I\t18\t19\n",
    "J\t20\t21\n",
    "Now, let's go through the steps of the LOF algorithm:\n",
    "\n",
    "Step 1: Calculating Local Reachability Density (LRD):\n",
    "\n",
    "We compute the reachability distance and LRD for each instance based on its k nearest neighbors.\n",
    "\n",
    "For instance A (2, 3), its 3 nearest neighbors are B, C, and D. We calculate the reachability distance for each neighbor:\n",
    "\n",
    "Reachability distance from A to B: max(d(B, A), reach-dist(C, A), reach-dist(D, A)) = max(√13, √41, √85) = √85\n",
    "Reachability distance from A to C: max(d(C, A), reach-dist(B, A), reach-dist(D, A)) = max(√85, √37, √85) = √85\n",
    "Reachability distance from A to D: max(d(D, A), reach-dist(B, A), reach-dist(C, A)) = max(√85, √85, √85) = √85\n",
    "Now, we calculate the average reachability distance of the 3 nearest neighbors:\n",
    "LRD(A) = 1 / (average of reachability distances of nearest neighbors) = 1 / (√85) ≈ 0.116\n",
    "\n",
    "Similarly, we calculate the LRD values for all instances:\n",
    "\n",
    "Instance\tLRD\n",
    "A\t0.116\n",
    "B\t0.116\n",
    "C\t0.116\n",
    "D\t0.116\n",
    "E\t0.375\n",
    "F\t0.375\n",
    "G\t0.375\n",
    "H\t0.375\n",
    "I\t0.375\n",
    "J\t0.375\n",
    "Step 2: Calculating Local Outlier Factor (LOF):\n",
    "\n",
    "We compute the LOF for each instance based on its k nearest neighbors' LRD values.\n",
    "\n",
    "For instance A, its 3 nearest neighbors are B, C, and D. We calculate the LRD ratio for each neighbor:\n",
    "\n",
    "LRD ratio of B: LRD(B) / LRD(A) = 0.116 / 0.116 = 1\n",
    "LRD ratio of C: LRD(C) / LRD(A) = 0.116 / 0.116 = 1\n",
    "LRD ratio of D: LRD(D) / LRD(A) = 0.116 / 0.116 = 1\n",
    "The LOF value for instance A is the average of the LRD ratios of its nearest neighbors:\n",
    "LOF(A) = (1 + 1 + 1) / 3 = 1\n",
    "\n",
    "Similarly, we calculate the LOF values for all instances:\n",
    "\n",
    "Instance\tLOF\n",
    "A\t1\n",
    "B\t1\n",
    "C\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8b92bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21]]\n",
      "Instance 1: Anomaly Score = -1\n",
      "Instance 2: Anomaly Score = -1\n",
      "Instance 3: Anomaly Score = -1\n",
      "Instance 4: Anomaly Score = -1\n",
      "Instance 5: Anomaly Score = -1\n",
      "Instance 6: Anomaly Score = -1\n",
      "Instance 7: Anomaly Score = -1\n",
      "Instance 8: Anomaly Score = -1\n",
      "Instance 9: Anomaly Score = -1\n",
      "Instance 10: Anomaly Score = -1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Sample dataset\n",
    "X = [[2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15], [16, 17], [18, 19], [20, 21]]\n",
    "print(X)\n",
    "# Create LOF instance with k=3\n",
    "lof = LocalOutlierFactor(n_neighbors=3)\n",
    "\n",
    "# Compute anomaly scores\n",
    "anomaly_scores = -lof.fit_predict(X)\n",
    "\n",
    "# Print anomaly scores\n",
    "for i, score in enumerate(anomaly_scores):\n",
    "    print(f\"Instance {i+1}: Anomaly Score = {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf239dc1",
   "metadata": {},
   "source": [
    "## Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162acbc",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has several key parameters that can be adjusted to control its behavior and performance. Here are the main parameters of the Isolation Forest algorithm:\n",
    "\n",
    "- **n_estimators:** This parameter specifies the number of isolation trees to be built. Increasing the number of trees can improve the performance and accuracy of the algorithm, but it also increases the computational complexity. It is typically set based on the size and complexity of the dataset.\n",
    "\n",
    "- **max_samples:** This parameter determines the number of samples to be used for building each isolation tree. It can be set as a fixed number or a fraction of the total number of instances in the dataset. Higher values can increase the randomness and diversity of the trees, but it also increases the memory and computational requirements.\n",
    "\n",
    "- **max_features:** This parameter controls the number of features to be considered when splitting a node in the isolation tree. It can be set as a fixed number or a fraction of the total number of features. Smaller values can increase the randomness and diversity of the trees, but it may result in less effective splits.\n",
    "\n",
    "- **contamination:** This parameter specifies the expected proportion of anomalies or outliers in the dataset. It is used to define the threshold for classifying instances as anomalies. By default, it is set to \"auto,\" which estimates the contamination based on the dataset's characteristics. It can also be set to a specific value if prior knowledge about the contamination level is available.\n",
    "\n",
    "- **random_state:** This parameter sets the random seed used by the algorithm for reproducibility. By setting a fixed random state, you can obtain consistent results when running the algorithm multiple times.\n",
    "\n",
    "These are the primary parameters that influence the behavior and performance of the Isolation Forest algorithm. Selecting appropriate parameter values depends on the specific dataset, the nature of anomalies, and the desired trade-off between performance and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2fe975",
   "metadata": {},
   "source": [
    "## Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c74740",
   "metadata": {},
   "source": [
    "- In this case, since there are only 2 neighbors within the radius, and both of them belong to the same class, the proportion of neighbors of the same class is 2/2 = 1. Therefore, we can say that the data point has a 100% proportion of neighbors of the same class within the given radius.\n",
    "\n",
    "- Based on this information, it is likely that the data point will have a lower anomaly score since it has neighbors of the same class within the radius, indicating that it aligns well with its immediate surroundings. However, the exact calculation and interpretation of the anomaly score can vary depending on the specific algorithm and implementation used in the KNN-based anomaly detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186fc9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.8\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'k' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m anomaly_score \u001b[38;5;241m=\u001b[39m ((K \u001b[38;5;241m-\u001b[39m same_class_neighbors) \u001b[38;5;241m/\u001b[39m K)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnomaly Score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, anomaly_score)\n\u001b[1;32m----> 7\u001b[0m anomaly_score \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m (same_class_neighbors \u001b[38;5;241m/\u001b[39m \u001b[43mk\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k' is not defined"
     ]
    }
   ],
   "source": [
    "same_class_neighbors = 2\n",
    "K = 10\n",
    "# Calculate the anomaly score\n",
    "anomaly_score = (K - same_class_neighbors) / K\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)\n",
    "anomaly_score = 1.0 - (same_class_neighbors / k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e804c495",
   "metadata": {},
   "source": [
    "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09dd09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.795724283075825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Average path length and number of trees\n",
    "average_path_length = 5.0\n",
    "n_trees = 100\n",
    "n_samples = 3000\n",
    "\n",
    "# Calculate the constant 'c'\n",
    "c = 2 * (np.log(n_samples - 1) + np.euler_gamma) - (2 * (n_samples - 1) / n_samples)\n",
    "\n",
    "# Calculate the anomaly score\n",
    "anomaly_score = 2.0 ** (-average_path_length / c)\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

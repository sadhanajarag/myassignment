{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a8a7333",
   "metadata": {},
   "source": [
    "## Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "## Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a788ad9d",
   "metadata": {},
   "source": [
    "- Eigenvalues and eigenvectors are mathematical concepts used in linear algebra to study the properties of matrices. In the context of the eigen-decomposition approach, eigenvalues and eigenvectors play a crucial role in decomposing a matrix into simpler components.\n",
    "\n",
    "1. Eigenvalues:  Eigenvalues are a scalar value that represents how an eigenvector is stretched or shrunk by : a linear transformation represented by a matrix. \n",
    "\n",
    "2. Eigenvector: An eigenvector is a non-zero vector that, when multiplied by a matrix, produces a scalar multiple of itself, i.e., it only changes in magnitude, not in direction.\n",
    "\n",
    "Note : Multiplying a matrix by a vector can also be interpreted as a linear transformation. In most cases, this transformation will change the direction of the vector.\n",
    "\n",
    "Let’s assume, we have a matrix (A) and a vector (v), which we can multiply.\n",
    "\n",
    "Av = [[1,1],[0,1][[3],[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c50efde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:524/format:webp/1*3r8j4FG_pLPszzl9BAxkVA.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://miro.medium.com/v2/resize:fit:524/format:webp/1*3r8j4FG_pLPszzl9BAxkVA.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeabadb",
   "metadata": {},
   "source": [
    "- It turns out when performing eigendecomposition, we are looking for a vector, whose direction will not be changed by a matrix-vector multiplication — only its magnitude will either be scaled up or down.\n",
    "- Therefore, the effect of the matrix on the vector is the same as the effect of a scalar on the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440f0e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*8ycv_f7snhZ6jbHXvjvt0g.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://miro.medium.com/v2/resize:fit:828/format:webp/1*8ycv_f7snhZ6jbHXvjvt0g.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adb7051",
   "metadata": {},
   "source": [
    "- This effect can be described, more formally, by the fundamental eigenvalue equation:\n",
    "            Av=λv\n",
    "- After rearranging and factoring the vector (v) out, we get the following equation:\n",
    "            (A-λI)=0\n",
    "- Now, we arrived at the core idea of eigendecomposition. In order to find a non-trivial solution to the equation above, we first need to discover the scalars (λ), that shift the matrix (A) just enough, to make sure a matrix-vector multiplication equals zero, thus sending the vector (v) in its null-space.\n",
    "\n",
    "- Thinking about it geometrically, we are basically looking for a matrix that squishes space into a lower dimension with an area or volume of zero. We can achieve this squishing effect when the matrix determinant equals zero.\n",
    "        delta(A-λI)=0\n",
    "    where λ is a scalar called the eigenvalue associated to the eigenvector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8017004",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are fundamental concepts in linear algebra, and are closely related to the Eigen-Decomposition approach.\n",
    "\n",
    "In simple terms, an eigenvector of a square matrix is a non-zero vector that, when multiplied by the matrix, results in a scalar multiple of itself, known as an eigenvalue.\n",
    "\n",
    "For example, consider the following 2x2 matrix A:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0109a322",
   "metadata": {},
   "source": [
    "Step 1 :  A = [3 2]\n",
    "         [1 4]\n",
    "    \n",
    "- To find the eigenvectors and eigenvalues of A, we need to solve the equation:\n",
    "    \n",
    "                A * v = λ * v\n",
    "\n",
    "- where v is the eigenvector and λ is the eigenvalue. Rearranging the equation, we get:\n",
    "\n",
    "                (A - λ * I) * v = 0\n",
    "\n",
    "         where I is the identity matrix.\n",
    "        The above equation has non-zero solutions only if the determinant of (A - λ * I) is zero. \n",
    "        Therefore, we need to find the eigenvalues λ that satisfy the equation:\n",
    "    \n",
    "- det(A - λ * I) = 0\n",
    "\n",
    "- Solving the determinant equation, we get:\n",
    "\n",
    "                    λ1 = 2, λ2 = 5\n",
    "\n",
    "- These are the eigenvalues of A. To find the eigenvectors corresponding to these eigenvalues, we substitute the eigenvalues back into the equation (A - λ * I) * v = 0 and solve for v.\n",
    "\n",
    "                        For λ1 = 2, we get:\n",
    "\n",
    "                    A - 2 * I = [1 2]\n",
    "                                [1 2]\n",
    "\n",
    "- Substituting this into (A - λ * I) * v = 0, we get:\n",
    "    \n",
    "                    [1 2] * [x1] = 0\n",
    "                    [1 2]   [x2]\n",
    "\n",
    "                => x1 + 2x2 = 0\n",
    "                   x1 + 2x2 = 0\n",
    "\n",
    "- Solving this system of equations, we get x1 = -2x2. Choosing x2 = 1, we get the eigenvector:\n",
    "\n",
    "                    v1 = [-2       1]\n",
    "\n",
    "- Similarly, for λ2 = 5, we get:\n",
    "\n",
    "                        A - 5 * I = [-2  2]\n",
    "                                    [ 1 -1]\n",
    "            \n",
    "- Substituting this into (A - λ * I) * v = 0, we get:\n",
    "\n",
    "                    [-2  2] * [x1] = 0\n",
    "                    [ 1 -1]   [x2]\n",
    "\n",
    "                    => -2x1 + 2x2 = 0\n",
    "                         x1 - x2 = 0\n",
    "\n",
    "- Solving this system of equations, we get x1 = x2. Choosing x2 = 1, we get the eigenvector:\n",
    "                        v2 = [1     1]\n",
    "\n",
    "- Now that we have the eigenvalues and eigenvectors of A, we can use them to perform the Eigen-Decomposition. \n",
    "    The Eigen-Decomposition of A is given by:\n",
    "    \n",
    "                            A = V * Λ * V^-1\n",
    "\n",
    "\n",
    "- where V is the matrix whose columns are the eigenvectors of A, Λ is the diagonal matrix whose diagonal entries are the    eigenvalues of A, and V^-1 is the inverse of V.\n",
    "\n",
    "- Substituting the eigenvalues and eigenvectors we found earlier, we get:\n",
    "    where V is the matrix whose columns are the eigenvectors of A, Λ is the diagonal matrix whose diagonal entries are the eigenvalues of A, and V^-1 is the inverse of V.\n",
    "\n",
    "- Substituting the eigenvalues and eigenvectors we found earlier, we get:\n",
    "    \n",
    "                    A = [-2  1] * [2 0] * [ 1  1]\n",
    "                        [ 1  1]   [0 5]   [-1  2]\n",
    "    \n",
    "- This is the Eigen-Decomposition of A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03525806",
   "metadata": {},
   "source": [
    "## Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd75046",
   "metadata": {},
   "source": [
    "**Eigen decomposition, also known as spectral decomposition, is a process in linear algebra that decomposes a square matrix into its constituent eigenvectors and eigenvalues.**\n",
    "\n",
    "- An eigenvector is a non-zero vector that only changes in magnitude when multiplied by a matrix, while an eigenvalue is a scalar that represents the amount by which the eigenvector is scaled during multiplication.\n",
    "\n",
    "- The eigen decomposition of a matrix A involves finding a set of n linearly independent eigenvectors, v1, v2, ..., vn, and corresponding eigenvalues, λ1, λ2, ..., λn, such that:\n",
    "\n",
    "                        Avi = λivi\n",
    "\n",
    "- The significance of eigen decomposition in linear algebra is that it provides a useful tool for understanding the behavior of linear transformations and solving linear systems.\n",
    "\n",
    "- For example, eigenvectors can be used to identify the directions in which a transformation stretches or compresses space, while eigenvalues can be used to determine the amount of stretching or compression that occurs.\n",
    "\n",
    "- Eigen decomposition is also a fundamental step in many other mathematical operations, such as diagonalization, singular value decomposition, and principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48875dd9",
   "metadata": {},
   "source": [
    "### Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6737db77",
   "metadata": {},
   "source": [
    "- A square matrix A is diagonalizable using the Eigen-Decomposition approach if and only if it satisfies the following conditions:\n",
    "\n",
    "    1. A has n linearly independent eigenvectors, where n is the dimension of A.\n",
    "    2. A can be expressed as a product of its eigenvectors and eigenvalues.\n",
    "    \n",
    "Proof:\n",
    "\n",
    "First, suppose that A is diagonalizable. This means that there exists an invertible matrix P and a diagonal matrix D such that A = PDP^-1, where the columns of P are the eigenvectors of A and D contains the corresponding eigenvalues along the diagonal.\n",
    "\n",
    "Let vi be the ith eigenvector of A, with eigenvalue λi. Then we have:\n",
    "\n",
    "            Avi = λivi\n",
    "\n",
    "Multiplying both sides by P, we get:\n",
    "\n",
    "            AP = PD\n",
    "\n",
    "Expanding this equation, we have:\n",
    "\n",
    "            A[v1 v2 ... vn] = [v1 v2 ... vn] [λ1 0 ... 0; 0 λ2 ... 0; ... ; 0 0 ... λn]\n",
    "\n",
    "This shows that A can be expressed as a product of its eigenvectors and eigenvalues, satisfying the second condition.\n",
    "\n",
    "- To show that A has n linearly independent eigenvectors, we can assume the contrary and suppose that A has fewer than n linearly independent eigenvectors. Let k be the number of linearly independent eigenvectors of A. Then we can choose k linearly independent eigenvectors, v1, v2, ..., vk, and express the remaining n-k eigenvectors as linear combinations of these k eigenvectors.\n",
    "\n",
    "Let S be the matrix whose columns are v1, v2, ..., vk. Then we have:\n",
    "\n",
    "                            AS = SΛ\n",
    "\n",
    "where Λ is a diagonal matrix containing the corresponding eigenvalues along the diagonal.\n",
    "\n",
    "            Multiplying both sides by S^-1, we get:\n",
    "\n",
    "                            S^-1AS = Λ\n",
    "\n",
    "This shows that A is similar to a diagonal matrix Λ. However, since A has fewer than n linearly independent eigenvectors, we cannot choose a matrix P with n linearly independent eigenvectors, which contradicts the assumption that A is diagonalizable. Therefore, A must have n linearly independent eigenvectors to be diagonalizable using the Eigen-Decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ae56d",
   "metadata": {},
   "source": [
    "## Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach? How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ea1e22",
   "metadata": {},
   "source": [
    "- The spectral theorem is a fundamental result in linear algebra that states that a symmetric matrix is diagonalizable using an orthonormal basis of eigenvectors. In other words, if A is a real symmetric matrix, then there exists an orthonormal basis of eigenvectors that diagonalizes A.\n",
    "\n",
    "- The spectral theorem is significant in the context of the Eigen-Decomposition approach because it provides a condition for when a matrix can be diagonalized using eigenvectors. Specifically, if a matrix A is symmetric, then it can be diagonalized using the Eigen-Decomposition approach with an orthonormal basis of eigenvectors, and the corresponding eigenvalues are real.\n",
    "\n",
    "- The relationship between the spectral theorem and the diagonalizability of a matrix can be illustrated with an example. Consider the following symmetric matrix:\n",
    "\n",
    "                        A =\n",
    "                            [ 2 1 1 ]\n",
    "                            [ 1 2 1 ]\n",
    "                            [ 1 1 2 ]\n",
    "\n",
    "- To determine if A can be diagonalized using the Eigen-Decomposition approach, we need to check if it has n linearly independent eigenvectors, where n is the dimension of A. Since A is a symmetric matrix, we know that it can be diagonalized using an orthonormal basis of eigenvectors according to the spectral theorem. Therefore, we can simply find the eigenvectors and eigenvalues of A to determine if it is diagonalizable.\n",
    "\n",
    "- To find the eigenvectors and eigenvalues of A, we solve the equation Av = λv, where v is a non-zero vector and λ is a scalar. This gives us the following eigenvalues and eigenvectors:\n",
    "\n",
    "                        λ1 = 1, v1 = [1, -1, 0]\n",
    "                        λ2 = 3, v2 = [1, 1, -2]\n",
    "                        λ3 = 2, v3 = [1, 1, 1]\n",
    "\n",
    "- Since A has n linearly independent eigenvectors, we can use the Eigen-Decomposition approach to diagonalize A. We can form the matrix P whose columns are the eigenvectors of A, and the diagonal matrix D whose diagonal entries are the corresponding eigenvalues:\n",
    "\n",
    "                            P =[ 1 1 1 ]\n",
    "                                [ -1 1 1 ]\n",
    "                                    [ 0 -2 1 ]\n",
    "\n",
    "\n",
    "                                    D =[ 1 0 0 ]\n",
    "                                    [ 0 3 0 ]\n",
    "                                    [ 0 0 2 ]\n",
    "\n",
    "- Then, we have A = PDP^-1, where P^-1 = P^T since the eigenvectors of A are orthonormal. This shows that A can be diagonalized using the Eigen-Decomposition approach with an orthonormal basis of eigenvectors, as guaranteed by the spectral theorem for symmetric matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff20b97",
   "metadata": {},
   "source": [
    "## Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2f267",
   "metadata": {},
   "source": [
    "- To find the eigenvalues of a square matrix A, we solve the characteristic equation:\n",
    "\n",
    "                    det(A - λI) = 0\n",
    "\n",
    "where \n",
    "I is the identity matrix of the same size as A, \n",
    "and λ is a scalar. \n",
    "The solutions to this equation are the eigenvalues of A.\n",
    "\n",
    "- The eigenvalues of a matrix represent the scalar factors by which eigenvectors of the matrix are scaled when multiplied by the matrix. Specifically, an eigenvector of A with eigenvalue λ satisfies the equation:\n",
    "\n",
    "                        Av = λv\n",
    "\n",
    "where v is a non-zero vector. This means that when A is multiplied by the eigenvector v, the result is a scalar multiple of v, namely λ times v. \n",
    "In other words, the eigenvector is only scaled by a scalar factor, which is the eigenvalue λ.\n",
    "\n",
    "- Eigenvalues have many important applications in linear algebra and beyond. For example, they can be used to determine whether a matrix is invertible, to find the solutions to systems of linear differential equations, and to analyze the stability of dynamical systems. Eigenvalues are also used in many areas of science and engineering, such as physics, chemistry, and signal processing, to analyze and model various phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbcd36",
   "metadata": {},
   "source": [
    "## Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d47ef4a",
   "metadata": {},
   "source": [
    "- eigenvectors are non-zero vectors that, when multiplied by a square matrix A, are only scaled by a scalar factor. Specifically, an eigenvector v of a square matrix A satisfies the equation:\n",
    "\n",
    "                        Av = λv\n",
    "\n",
    "where λ is a scalar known as the eigenvalue corresponding to the eigenvector v.\n",
    "\n",
    "- The relationship between eigenvectors and eigenvalues is that every eigenvalue of a square matrix A corresponds to at least one eigenvector, and vice versa. In other words, for each eigenvalue λ of A, there exists at least one non-zero eigenvector v such that Av = λv, and conversely, for each eigenvector v of A, there exists at least one eigenvalue λ such that Av = λv.\n",
    "\n",
    "- Eigenvalues and eigenvectors are important concepts in linear algebra, as they can be used to diagonalize a matrix and simplify calculations involving the matrix. Specifically, if a matrix A has n linearly independent eigenvectors, where n is the dimension of A, then it can be diagonalized as A = PDP^-1, where P is a matrix whose columns are the eigenvectors of A, and D is a diagonal matrix whose diagonal entries are the corresponding eigenvalues. In this diagonalized form, many calculations involving the matrix A become simpler and easier to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af45a40",
   "metadata": {},
   "source": [
    "## Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea4f5a",
   "metadata": {},
   "source": [
    "We will first discuss eigenvectors and eigenvalues using conventional matrix notation although eigenvectors and eigenvalues are not specific to matrices and other algebras and notations can be used.\n",
    "\n",
    "The eigenvalues of a matrix [M] are the values of a vector 'v' such that:\n",
    "\n",
    "                        [M] v = λ v\n",
    "\n",
    "where:\n",
    "\n",
    "v = eigenvector\n",
    "λ = lambda = eigenvalue\n",
    "In other words if we treat the matrix 'M' as a transform which vectors are not changed (or are only scaled) by the matrix.\n",
    "\n",
    "One of the reasons that eigenvectors are so important is that the points that do not move are what defines the symmetry of a given operation\n",
    "\n",
    "So far the assumption is that the matrix contains real values, if the matrix is over complex numbers for example, then these results may be modified (a common geometric interpretation of the imaginary operator 'i' is rotation by 90°).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413bae97",
   "metadata": {},
   "source": [
    "## Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2cb25",
   "metadata": {},
   "source": [
    "- Image and signal processing: Eigenvectors and eigenvalues can be used to analyze images and signals in fields such as computer vision, medical imaging, and speech recognition. For example, the eigenvectors of a covariance matrix can be used to perform Principal Component Analysis (PCA), which is a popular technique for reducing the dimensionality of high-dimensional data.\n",
    "\n",
    "- Quantum mechanics: In quantum mechanics, the wave function of a physical system can be decomposed into a linear combination of eigenvectors of the Hamiltonian operator, which represents the energy of the system. This allows physicists to study the behavior of quantum systems and predict their future states.\n",
    "\n",
    "- Structural engineering: Eigenvectors and eigenvalues can be used to analyze the vibrational modes of structures such as buildings, bridges, and airplanes. The eigenvectors represent the modes of vibration, while the eigenvalues represent the frequencies at which the structure vibrates.\n",
    "\n",
    "- Finance: Eigen decomposition can be used in finance to analyze the covariance matrix of stock returns and to construct portfolios that optimize risk and return. For example, the eigenvectors of the covariance matrix can be used to construct a portfolio that maximizes diversification and minimizes risk.\n",
    "\n",
    "- Machine learning: Eigenvectors and eigenvalues are used in many machine learning algorithms, such as PCA, Singular Value Decomposition (SVD), and the Eigenfaces algorithm for facial recognition. These algorithms can be used for tasks such as image compression, data visualization, and feature extraction.\n",
    "\n",
    "Overall, eigen decomposition is a powerful tool for analyzing and understanding complex data and systems in many different fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a845278",
   "metadata": {},
   "source": [
    "## Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb2c3a",
   "metadata": {},
   "source": [
    "- In general, a matrix can have multiple sets of eigenvectors and eigenvalues, but the sets will be distinct only if the matrix is not diagonalizable.\n",
    "\n",
    "- If a matrix is diagonalizable, then it will have a unique set of eigenvectors (up to scalar multiples) and corresponding eigenvalues. However, if the matrix is not diagonalizable, then it may have fewer eigenvectors than its dimension, and it may have repeated eigenvalues.\n",
    "\n",
    "- For example, consider the matrix A = [1 1; 0 1]. The characteristic polynomial of A is λ^2 - 2λ + 1, which has a repeated root of λ = 1. Therefore, A has only one distinct eigenvalue, and it turns out that A has only one eigenvector, which is [1; 0].\n",
    "\n",
    "- On the other hand, consider the matrix B = [0 1; -1 0]. The characteristic polynomial of B is λ^2 + 1, which has complex roots of λ = ±i. Therefore, B has two distinct eigenvalues, both of which are complex. It turns out that the eigenvectors of B are [1; i] and [1; -i], which are distinct but not orthogonal.\n",
    "\n",
    "- In summary, a matrix can have multiple sets of eigenvectors and eigenvalues if it is not diagonalizable, or if it has repeated eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939f3a3",
   "metadata": {},
   "source": [
    "## Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning? Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45b7e5",
   "metadata": {},
   "source": [
    "- The Eigen-Decomposition approach is useful in data analysis and machine learning because it can provide a compact and interpretable representation of high-dimensional data, and it can reveal underlying patterns and structures in the data. Here are three specific applications or techniques that rely on Eigen-Decomposition:\n",
    "\n",
    "- Principal Component Analysis (PCA): PCA is a popular technique for reducing the dimensionality of high-dimensional data while retaining most of the important information. The basic idea of PCA is to find the eigenvectors and eigenvalues of the covariance matrix of the data, and then use them to transform the data into a new coordinate system where the principal components (i.e., the directions of maximum variance) are aligned with the coordinate axes. This transformation can help to visualize and analyze the data, and it can also be used for tasks such as data compression, feature extraction, and anomaly detection.\n",
    "\n",
    "- Singular Value Decomposition (SVD): SVD is a generalization of the Eigen-Decomposition approach that can be applied to any rectangular matrix, not just square matrices. SVD can be used for tasks such as matrix factorization, image compression, and data reconstruction. The basic idea of SVD is to decompose a matrix into three matrices: a left singular matrix, a diagonal matrix of singular values, and a right singular matrix. The singular values represent the strength of the correlations between the rows and columns of the matrix, and the left and right singular matrices represent the directions of the strongest correlations.\n",
    "\n",
    "- Eigenfaces: Eigenfaces is a classic facial recognition algorithm that uses the Eigen-Decomposition approach to represent faces as linear combinations of eigenvectors. The basic idea of Eigenfaces is to find the eigenvectors and eigenvalues of the covariance matrix of a set of face images, and then use them to represent each face as a weighted sum of the eigenvectors. The weights are then used as features for classification, and the algorithm can recognize faces even under varying lighting and pose conditions. Eigenfaces is a simple yet powerful technique that has inspired many other variants and extensions in the field of computer vision.\n",
    "\n",
    "Overall, the Eigen-Decomposition approach is a versatile and powerful tool for analyzing high-dimensional data and revealing underlying patterns and structures. It has many applications in data analysis, machine learning, and other fields, and it continues to inspire new research and innovations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

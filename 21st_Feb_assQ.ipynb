{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a51a8a9-e52b-419e-ad1f-3413cb53a556",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dbd34-96b8-42ef-a1e1-fd92badc5f26",
   "metadata": {},
   "source": [
    "Def :\n",
    "    Web scraping refers to the extraction of data from a website.\n",
    "    Web scraping is an automatic method to obtain large amounts of data from websites.\n",
    "    Most of this data is unstructured data in an HTML format which is then converted into structured \n",
    "    data in a spreadsheet or a database so that it can be used in various applications.\n",
    "    \n",
    "why ?\n",
    "    :\n",
    "Automation:\n",
    "            The first and most important benefit of web scraping is developing tools that have simplified data retrieval from different websites to only a few clicks. Data could still be extracted before this approach, but it was a tedious and time-consuming process.\n",
    "\n",
    "Cost-Effective:\n",
    "            Data extraction by hand is an expensive task that necessitates a large workforce and large budgets. Nonetheless, web scraping, like many other digital techniques, has solved this problem.\n",
    "\n",
    "Easy Implementation:\n",
    "            When a website scraping service begins gathering data, you should be confident that you are obtaining data from various websites, not just a single page. It is possible to have a large volume of data with a small investment to help you get the best out of that data.\n",
    "\n",
    "Low Maintenance:\n",
    "            When it comes to maintenance, the cost is something that is often ignored when installing new services. Fortunately, web scraping technologies need little to no maintenance over time. So, in the long run, services and budgets will not undergo drastic changes in terms of maintenance.\n",
    "\n",
    "Speed:\n",
    "        Another feature worth mentioning is the speed with which web scraping services complete actions. Imagine that a scraping project that would typically take weeks is completed in a matter of hours. But of course, that depends on the complexity of the projects, resources, and tools used.\n",
    "\n",
    "Data Accuracy:\n",
    "        Web scraping services are not only speed obsessive but also accurate. It’s a fact that human error is often a factor when performing a task manually, and that can lead to more serious problems later on. As a result, accurate data extraction for any type of information is critical.Human error is often a factor when performing a task manually, as we all know, and that can lead to more serious problems later on. But when it comes to web scraping, this cannot happen. Or it happens at least in very small proportions, which can be easily corrected.\n",
    "\n",
    "Effective Management of Data:\n",
    "        By storing data with automated software and programs, your company or employees will be able to spend no time copying and pasting data. So they can focus more time on creative work, for example.Instead of this tedious work, web scraping allows you to pick and choose which data you want to collect from various websites and then use the right tools to collect it properly. Moreover, using automated software and programs to store data ensures that your information is secure.\n",
    "        \n",
    "\n",
    "three areas where we use web scrapping:\n",
    "\n",
    "1. In eCommerce, Web Scraping is used for competition price monitoring.\n",
    "2. In Marketing, Web Scraping is used for lead generation, to build phone and email lists for cold outreach.\n",
    "3. In Real Estate, Web Scraping is used to get property and agent/owner details.\n",
    "4. Web Scraping is used to collect training and testing data for Machine Learning projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef6e7c2-5e1f-41c7-b4d5-bf4c5bc21749",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdfb7ed-3d67-40a2-9f95-16541ee279dc",
   "metadata": {},
   "source": [
    "Web scraping techniques\n",
    "\n",
    "\n",
    "1) Human copy and paste involve copying specific data from the web and pasting it into a text file or spreadsheet manually.\n",
    "\n",
    "2) Web scraping with Python uses Python’s regular expression-matching abilities to extract information from web pages. Data science professionals and programmers also use programming languages like Ruby, Java, C++, and JavaScript for web scraping.\n",
    "\n",
    "3) Document object model (DOM) parsing embeds web browsers to scrape the dynamic content that client-side scripts generate.\n",
    "\n",
    "4) Semantic annotation recognition uses semantic markups or metadata to locate and extract data snippets.\n",
    "\n",
    "5) Computer vision-aided analysis extracts data from web pages with the help of machine learning and computer vision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c8db4-9747-47b3-bf18-cccb64d57a68",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc5ceaa-086b-460a-8261-643c443e7541",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in HTML, XML files.\n",
    "It transforms a complex HTML document into a tree of Python objects.\n",
    "It also automatically converts the document to Unicode, so you don’t have to think about encodings. \n",
    "This tool not only helps you scrape but also to clean the data. \n",
    "Beautiful Soup supports the HTML parser included in Python’s standard library,\n",
    "\n",
    "why:\n",
    "    \n",
    "Many of your coding projects may require you to pull a bunch of information from an HTML or XML page.\n",
    "This task can be really tedious and boring, that is until you learn how to scrape the web with an HTML Parser! \n",
    "That’s where Beautiful Soup comes in. This Python package allows you to parse HTML and XML pages with ease and pull all sorts of data off the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b2851-b113-4cad-9352-2e7825367308",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8229b-a3ba-4b64-821e-2801cd09a96e",
   "metadata": {},
   "source": [
    "Flask is a lightweight framework to build websites.\n",
    "We’ll use this to parse our collected data and display it as HTML in a new HTML file.\n",
    "The requests module allows us to send http requests to the website we want to scrape.\n",
    "This means flask provides you with tools, libraries and technologies that allow you to build a web application. This web application can be some web pages, a blog, a wiki or go as big as a web-based calendar application or a commercial website.\n",
    "we can build mny scrapping web application in easy way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22794654-85be-4f5d-af47-71350a5af089",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19410eab-37f5-4746-9c43-ed5ca97f1672",
   "metadata": {},
   "outputs": [],
   "source": [
    "We have used Amazon Elastic Beanstalk and AWS codepipeline services in this AWS project.\n",
    "We have get the code from the Git hub and via AWS codepipeline executed in the Amazon Elastic Beanstalk,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c659b-f6b0-4c3b-82e5-936c51e25d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Elastic Beasnstalk:\n",
    "    \n",
    "We need the system on clould side means PC so Amazon Elastic Beanstalk is a web infrastructure management service. \n",
    "It handles deployment and scaling for web applications and services.\n",
    "Elastic Beanstalk can automatically manage setup, configuration, scaling and provisioning for other AWS services.\n",
    "Elastic Beanstalk reduces management complexity without restricting choice or control. \n",
    "Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby.\n",
    "When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as Amazon EC2 instances, to run your application.\n",
    "Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. \n",
    "After your environment is launched, you can then manage your environment and deploy new application versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a07589-927f-4dc1-8748-d47034b2b6fa",
   "metadata": {},
   "source": [
    "AWS code Pipeline:\n",
    "\n",
    "It is a fully managed continuous delivery service that helps you automate your release pipelines for application and infrastructure updates.\n",
    "You can easily integrate AWS CodePipeline with third-party services such as GitHub or with your own custom plugin.\n",
    "    \n",
    "A pipeline defines your release process workflow, and describes how a new code change progresses through your release process.\n",
    "\n",
    "A pipeline comprises a series of stages (e.g., build, test, and deploy), which act as logical divisions in your workflow.\n",
    "Pipelines must have at least two stages. The first stage of a pipeline is required to be a source stage, and the pipeline is required to additionally have at least one other stage that is a build or deployment stage.\n",
    "\n",
    "AWS CodePipeline provides you with a graphical user interface to create, configure, and manage your pipeline and its various stages and actions.\n",
    "A pipeline starts automatically (default) when a change is made in the source location, or when you manually start the pipeline.\n",
    ".\n",
    "You can model your build, test, and deployment actions to run in parallel in order to increase your workflow speeds.\n",
    "\n",
    "AWS CodePipeline can pull source code for your pipeline directly from AWS CodeCommit, GitHub.\n",
    "\n",
    "It can deploy your changes using AWS Elastic Beanstalk, AWS Fargate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53401b58-a1cf-4019-9727-2b11b692d888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813dc606-e621-4881-a305-4c1f18b33624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23496cc-a785-4258-8487-0eb1d663cdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03654be1",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d67f89",
   "metadata": {},
   "source": [
    "The Euclidean distance metric and the Manhattan distance metric are two popular distance metrics used in K-Nearest Neighbor (KNN) algorithms.\n",
    "\n",
    "- The main difference between them is the way they measure distance. The Euclidean distance is the straight-line distance between two points in a Euclidean space, while the Manhattan distance is the sum of the absolute differences of the coordinates.\n",
    "\n",
    "- To illustrate, suppose we have two points in a 2D space, A(3,4) and B(1,2). The Euclidean distance between them is √((3-1)² + (4-2)²) = √8 ≈ 2.83, while the Manhattan distance is |3-1| + |4-2| = 4.\n",
    "\n",
    "- The choice of distance metric can affect the performance of a KNN classifier or regressor. The Euclidean distance metric is suitable for continuous data and works well when the features are normalized and have the same scale. \n",
    "- On the other hand, the Manhattan distance metric is suitable for categorical or discrete data and works well when the features have different units of measurement.\n",
    "\n",
    "- Manhattan Distance is the L1 norm form (L1 norm is the sum of the magnitude of vectors in space)\n",
    "- Euclidean Distance is L2 Norm form (The L2 norm calculates the distance of the vector coordinate from the origin of the vector space.\n",
    "\n",
    "- In some cases, the Euclidean distance can be affected by outliers, as it gives more weight to large differences between coordinates. \n",
    "- The Manhattan distance, on the other hand, is less sensitive to outliers because it only measures the absolute differences between coordinates.\n",
    "\n",
    "Therefore, the choice of distance metric should be based on the characteristics of the data and the problem at hand. In some cases, it may be beneficial to try different distance metrics and choose the one that gives the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08871a6a",
   "metadata": {},
   "source": [
    "**Some of the use cases below saya that different distance measure effect on the performance of KNN:**\n",
    "\n",
    "- The Euclidean Distance tool is used frequently as a stand-alone tool for applications, such as finding the nearest hospital for an emergency helicopter flight. Alternatively, this tool can be used when creating a suitability map, when data representing the distance from a certain object is needed.\n",
    "- We don't use Manhattan Distance, because it calculates distance horizontally or vertically only. It has dimension restrictions. On the other hand, the Euclidean metric can be used in any space to calculate distance. Since the data points can be represented in any dimension, it is a more viable option.\n",
    "\n",
    "- Manhattan distance is usually preferred over the more common Euclidean distance when there is high dimensionality in the data.\n",
    "\n",
    "- KNN to classify test examples with the highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.\n",
    "- Euclidean distance function performs reasonably well over the categorical and numerical datasets, but not for the mixed type of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ab040",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d5c581",
   "metadata": {},
   "source": [
    "The optimal value of k in K-Nearest Neighbor (KNN) classification or regression depends on the specific dataset and the problem at hand. A small value of k can lead to overfitting, whereas a large value of k can lead to underfitting. Thus, selecting the right value of k is crucial to achieve the best performance of KNN.\n",
    "\n",
    "Here are some techniques that can be used to determine the optimal k value:\n",
    "\n",
    "1. Grid Search: One way to choose the optimal k value is to perform a grid search over a range of k values. In this approach, we define a set of k values and evaluate the KNN model's performance for each value of k. We then select the value of k that yields the best performance on a validation set or using cross-validation.\n",
    "\n",
    "2. Cross-validation: Cross-validation is a technique used to evaluate a machine learning model's performance on an independent dataset. In KNN, we can use cross-validation to determine the optimal value of k. We can split our dataset into multiple folds, and for each fold, we can train a KNN model with a different value of k. We then evaluate the performance of each KNN model on the remaining folds and average the results to get an estimate of the model's performance. We can repeat this process for different values of k and select the k that gives the best performance.\n",
    "\n",
    "3. Elbow Method: The elbow method is a graphical technique used to determine the optimal value of k. In this approach, we plot the performance metric against different values of k, and we look for a point where the performance stops improving significantly. This point is called the elbow point, and it represents the optimal value of k.\n",
    "\n",
    "4. Domain Knowledge: Sometimes, domain knowledge can help us choose the optimal value of k. For example, if we are working on a problem where we expect the data to have a lot of noise, we may choose a smaller value of k to reduce the effect of noise. Similarly, if we are working on a problem where we expect the data to have a lot of structure, we may choose a larger value of k to capture the underlying structure.\n",
    "\n",
    "In summary, selecting the optimal value of k for KNN requires a combination of experimentation and domain knowledge. We can use techniques like grid search, cross-validation, and the elbow method to determine the optimal value of k, but we should also consider the problem's specific characteristics to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02cc6a",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2478d42",
   "metadata": {},
   "source": [
    "The choice of distance metric plays a significant role in the performance of a K-Nearest Neighbor (KNN) classifier or regressor. The distance metric determines how the similarity between two data points is measured, and this affects how the KNN algorithm identifies the k nearest neighbors.\n",
    "\n",
    "In general, the distance metric should be chosen based on the data type and characteristics of the problem. Here are some situations in which one distance metric may be preferred over the other:\n",
    "\n",
    "- Euclidean distance: The Euclidean distance is commonly used for continuous data with homogeneous features. It works well when the features are normalized and have the same scale. However, it can be sensitive to outliers and large differences between feature values.\n",
    "\n",
    "- Manhattan distance: The Manhattan distance is commonly used for discrete data, such as text or categorical data. It works well when the features have different units of measurement and are not normalized. It is also less sensitive to outliers than the Euclidean distance.\n",
    "\n",
    "- Cosine similarity: Cosine similarity is commonly used for text data or high-dimensional data. It measures the cosine of the angle between two vectors and is insensitive to the magnitude of the vectors. Cosine similarity is useful when the direction of the data vectors is more important than their magnitude.\n",
    "\n",
    "- Mahalanobis distance: The Mahalanobis distance is commonly used when the data has correlated features or when the data has different scales. It takes into account the covariance between features and normalizes the distance metric accordingly.\n",
    "\n",
    "In summary, the choice of distance metric depends on the data type, the characteristics of the problem, and the desired performance. It is essential to experiment with different distance metrics and choose the one that gives the best performance for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923c9f0",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56452679",
   "metadata": {},
   "source": [
    "Hyperparameters are parameters that are set before the training process of a machine learning model and are not learned from the data. In K-Nearest Neighbor (KNN) classifiers and regressors, there are several hyperparameters that can affect the performance of the model. Here are some common hyperparameters in KNN models:\n",
    "\n",
    "1. K: The number of nearest neighbors to consider. A higher K can lead to smoother decision boundaries and less overfitting, but it can also lead to more bias and less sensitivity to local structures.\n",
    "\n",
    "2. Distance metric: The metric used to measure the distance between data points, such as Euclidean distance, Manhattan distance, or cosine similarity.\n",
    "\n",
    "3. Weighting: The weighting scheme used to combine the distances of the K nearest neighbors, such as uniform weighting or distance weighting.\n",
    "\n",
    "4. Leaf size: The number of samples stored in a leaf node of the KDTREE data structure used for efficient KNN search. A larger leaf size can lead to faster query times, but it can also lead to a lower accuracy.\n",
    "\n",
    "To tune these hyperparameters and improve the performance of the KNN model, one can use techniques such as grid search, randomized search, or Bayesian optimization. In grid search, a range of hyperparameter values is specified, and the model is trained and evaluated for each combination of hyperparameters. The combination that yields the best performance on a validation set is then selected. \n",
    "In randomized search, a random subset of hyperparameter values is sampled from the specified ranges, and the model is trained and evaluated for each combination.\n",
    "Bayesian optimization uses a probabilistic model to iteratively select hyperparameters that are expected to give the best performance, based on the previous evaluations.\n",
    "\n",
    "It is also essential to split the data into training, validation, and testing sets, to avoid overfitting and to assess the generalization performance of the model. Finally, it is recommended to use cross-validation to obtain a more robust estimate of the model's performance and to avoid overfitting to the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8daac0c",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e46c0",
   "metadata": {},
   "source": [
    "- The computational complexity of KNN increases with the size of the training dataset. For very large training sets, KNN can be made stochastic by taking a sample from the training dataset from which to calculate the K-most similar instances.\n",
    "\n",
    "Best Prepare Data for KNN\n",
    "\n",
    "- Rescale Data: KNN performs much better if all of the data has the same scale. Normalizing your data to the range [0, 1] is a good idea. It may also be a good idea to standardize your data if it has a Gaussian distribution.\n",
    "\n",
    "- Address Missing Data: Missing data will mean that the distance between samples can not be calculated. These samples could be excluded or the missing values could be imputed.\n",
    "\n",
    "- Lower Dimensionality: KNN is suited for lower dimensional data. You can try it on high dimensional data (hundreds or thousands of input variables) but be aware that it may not perform as well as other techniques. KNN can benefit from feature selection that reduces the dimensionality of the input feature space.\n",
    "\n",
    "- The optimal size of the training set depends on the complexity of the problem, the amount of available data, and the resources available for training the model. By using techniques such as cross-validation, learning curves, incremental training, data augmentation, and transfer learning, one can optimize the size of the training set and improve the performance of the machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58d1ed",
   "metadata": {},
   "source": [
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f129b95d",
   "metadata": {},
   "source": [
    "drawback of KNN :\n",
    "\n",
    "- KNN can be expensive in determining K if the dataset is large. It requires more memory storage than an effective classifier or supervised learning algorithms.\n",
    "- In KNN, the prediction phase is slow for a larger dataset. The computation of accurate distances plays a big role in determining the algorithm’s accuracy.\n",
    "- One of the major steps in KNN is determining the parameter K. Sometimes, it is unclear which type of distance to use and which feature will give the best result.\n",
    "- It is very sensitive to the data’s scale and irrelevant features. Irrelevant or correlated features have a high impact and must be eliminated.\n",
    "- The computation cost is quite high as each training example’s distance is calculated.\n",
    "- KNN is a lazy learning algorithm as it doesn’t learn from the training data; it simply memorizes it and then uses that data to classify the new input.\n",
    "- Typically difficult to handle high dimensionality\n",
    "\n",
    "There are several ways to improve the performance of the KNN model in machine learning:\n",
    "\n",
    "- Feature selection: KNN treats all features equally, so it is important to identify the most relevant features for the problem at hand. Feature selection techniques such as correlation-based feature selection, mutual information-based feature selection, and recursive feature elimination can be used to identify the most important features.\n",
    "\n",
    "- Distance metric selection: The choice of distance metric can have a significant impact on the performance of the KNN model. It is important to choose a distance metric that is appropriate for the data and the problem at hand. Popular distance metrics include Euclidean distance, Manhattan distance, and cosine similarity.\n",
    "\n",
    "- Data normalization: KNN is sensitive to differences in scale and range between features. Normalizing the data to have zero mean and unit variance can help to improve the performance of the KNN model.\n",
    "\n",
    "- Cross-validation: Cross-validation can be used to estimate the generalization performance of the KNN model and select the optimal hyperparameters, such as the number of neighbors (k) and the distance metric.\n",
    "\n",
    "- Ensemble methods: Ensemble methods such as Bagging and Boosting can be used to combine multiple KNN models and improve the overall performance.\n",
    "\n",
    "- Approximate nearest neighbors: Approximate nearest neighbor algorithms such as locality-sensitive hashing (LSH) can be used to reduce the computational complexity of KNN, while still maintaining good performance.\n",
    "\n",
    "- Outlier detection: Outliers can have a significant impact on the performance of the KNN model. Identifying and removing outliers can help to improve the performance of the KNN model.\n",
    "\n",
    "In summary, to improve the performance of the KNN model in machine learning, one can use feature selection techniques, choose an appropriate distance metric, normalize the data, use cross-validation to select the optimal hyperparameters, use ensemble methods, use approximate nearest neighbor algorithms, and detect and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4239d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
